{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "import os\n",
    "import plotnine as p9\n",
    "from skimpy import skim\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import warnings\n",
    "\n",
    "# To suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Extract legend using API\n",
    "client = Socrata(\"analisi.transparenciacatalunya.cat\", None)\n",
    "# Metadata\n",
    "\n",
    "# Metadades variables meteorològiques \n",
    "variables = client.get(\"4fb2-n3yi\", limit=2000)\n",
    "variables_legend = pd.DataFrame.from_records(variables).sort_values('codi_variable').reset_index(drop = True)\n",
    "\n",
    "# Metadades estacions meteorològiques automàtiques\n",
    "stations = client.get(\"yqwd-vj5e\", limit=2000)\n",
    "station_legend = pd.DataFrame.from_records(stations)\n",
    "\n",
    "# translate \n",
    "variables_legend['nom_variable_esp'] = variables_legend['nom_variable']\n",
    "# Define the translation dictionary\n",
    "translation_dict = {\n",
    "    'Pressió atmosfèrica màxima': 'Maximum Atmospheric Pressure',\n",
    "    'Pressió atmosfèrica mínima': 'Minimum Atmospheric Pressure',\n",
    "    'Humitat relativa màxima': 'Maximum Relative Humidity',\n",
    "    'Velocitat del vent a 10 m (esc.)': 'Wind Speed at 10 m (scaled)',\n",
    "    'Direcció de vent 10 m (m. 1)': 'Wind Direction at 10 m (m. 1)',\n",
    "    'Temperatura': 'Temperature',\n",
    "    'Humitat relativa': 'Relative Humidity',\n",
    "    'Pressió atmosfèrica': 'Atmospheric Pressure',\n",
    "    'Precipitació': 'Precipitation',\n",
    "    'Irradiància solar global': 'Global Solar Irradiance',\n",
    "    'Gruix de neu a terra': 'Snow Depth on Ground',\n",
    "    'Temperatura màxima': 'Maximum Temperature',\n",
    "    'Temperatura mínima': 'Minimum Temperature',\n",
    "    'Humitat relativa mínima': 'Minimum Relative Humidity',\n",
    "    'Velocitat del vent a 2 m (esc.)': 'Wind Speed at 2 m (scaled)',\n",
    "    'Direcció del vent a 2 m (m. 1)': 'Wind Direction at 2 m (m. 1)',\n",
    "    'Velocitat del vent a 6 m (esc.)': 'Wind Speed at 6 m (scaled)',\n",
    "    'Direcció del vent a 6 m (m. 1)': 'Wind Direction at 6 m (m. 1)',\n",
    "    'Ratxa màxima del vent a 10 m': 'Maximum Wind Gust at 10 m',\n",
    "    'Direcció de la ratxa màxima del vent a 10 m': 'Direction of Maximum Wind Gust at 10 m',\n",
    "    'Ratxa màxima del vent a 6 m': 'Maximum Wind Gust at 6 m',\n",
    "    'Direcció de la ratxa màxima del vent a 6 m': 'Direction of Maximum Wind Gust at 6 m',\n",
    "    'Ratxa màxima del vent a 2 m': 'Maximum Wind Gust at 2 m',\n",
    "    'Direcció de la ratxa màxima del vent a 2 m': 'Direction of Maximum Wind Gust at 2 m',\n",
    "    'Irradiància neta': 'Net Irradiance',\n",
    "    'Precipitació màxima en 1 minut': 'Maximum Precipitation in 1 minute'\n",
    "}\n",
    "\n",
    "variables_legend['nom_variable'] = variables_legend['nom_variable_esp'].map(translation_dict)\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "# start by plotting our stations that contain values that will be analysed\n",
    "station_legend['codi_comarca'] = station_legend['codi_comarca'].astype(str).str.zfill(2)\n",
    "\n",
    "station_geometry = station_legend[['codi_estacio','latitud','longitud','codi_comarca']]\n",
    "station_geometry = gpd.GeoDataFrame(station_geometry, geometry=gpd.points_from_xy(station_geometry['longitud'], station_geometry['latitud']))\n",
    "station_geometry.latitud = pd.to_numeric(station_geometry.latitud)\n",
    "station_geometry.longitud = pd.to_numeric(station_geometry.longitud)\n",
    "\n",
    "pc_muni_com_at_df = (pd.read_csv('data/geo_codes/pc_muni_com_at_codes.csv')\n",
    "                     .assign(com_code=lambda dd: dd.com_code.astype(str).str.zfill(2))\n",
    "                     )\n",
    "muni_com_df = (pc_muni_com_at_df\n",
    "               [['muni_code', 'muni_name', 'com_code', 'com_name']]\n",
    "               .drop_duplicates()\n",
    "               .assign(muni_code=lambda dd: dd.muni_code.astype(str).str.zfill(6))\n",
    ")\n",
    "\n",
    "at_muni_df = (pc_muni_com_at_df\n",
    "               [['muni_code', 'muni_name', 'at_code', 'at_name']]\n",
    "               .drop_duplicates()\n",
    "               .assign(muni_code=lambda dd: dd.muni_code.astype(str).str.zfill(6)))\n",
    "\n",
    "muni_shapes = (gpd.read_file('data/shapefiles/municipis/divisions-administratives-v2r1-municipis-50000-20230707.shp')\n",
    "    .rename(columns={'CODIMUNI': 'muni_code'})\n",
    "    [['muni_code', 'geometry']]\n",
    ")\n",
    "\n",
    "com_shapes = (muni_shapes\n",
    " .merge(muni_com_df, validate='one_to_one')\n",
    " .assign(geometry=lambda dd: dd['geometry'].buffer(0.001))\n",
    " .dissolve(by='com_code')\n",
    " .reset_index()\n",
    " .assign(com_code=lambda dd: dd['com_code'].astype(str).str.zfill(2))\n",
    " [['com_code', 'com_name', 'geometry']]\n",
    ")\n",
    "\n",
    "at_shapes = (muni_shapes\n",
    " .merge(at_muni_df)\n",
    " .assign(geometry=lambda dd: dd['geometry'].buffer(0.001))\n",
    " .dissolve(by='at_code')\n",
    " .reset_index()\n",
    " [['at_code', 'at_name', 'geometry']]\n",
    ")\n",
    "\n",
    "# convert to the same crs\n",
    "muni_shapes_latlong = muni_shapes.to_crs(epsg=4326)\n",
    "at_shapes_latlong = at_shapes.to_crs(epsg=4326)\n",
    "com_shapes_latlong = com_shapes.to_crs(epsg=4326)\n",
    "\n",
    "com_shapes_latlong_with_points = com_shapes_latlong[com_shapes_latlong['com_code'].isin(station_geometry['codi_comarca'].dropna().unique())]\n",
    "\n",
    "# display(p9.ggplot(muni_shapes_latlong) +\n",
    "#  p9.geom_map(alpha=0.1, size=.1) +\n",
    "#  p9.geom_map(data=com_shapes_latlong_with_points, alpha=0.4, size=.4, fill='lightgreen') +\n",
    "#  p9.geom_point(data=station_geometry, mapping=p9.aes(x='longitud', y='latitud'), size = 1) +\n",
    "#  p9.theme_void() +\n",
    "#  p9.labs(fill='') +\n",
    "#  p9.theme(figure_size=(6, 6),\n",
    "#             dpi=300,\n",
    "#             legend_position=(.65, .175)) +\n",
    "# p9.ggtitle('Meteorological Stations by comarca lvl.')\n",
    "#  )\n",
    "\n",
    "muni_shapes_latlong_with_points = gpd.sjoin(muni_shapes_latlong, station_geometry, how=\"inner\", op=\"intersects\")\n",
    "\n",
    "# display((p9.ggplot(muni_shapes_latlong) +\n",
    "#  p9.geom_map(alpha=0.1, size=.1) +\n",
    "#  p9.geom_map(data=muni_shapes_latlong_with_points, alpha=0.4, size=.4, fill='lightblue') +\n",
    "#  p9.geom_point(data=station_geometry, mapping=p9.aes(x='longitud', y='latitud'), size = 1) +\n",
    "#  p9.theme_void() +\n",
    "#  p9.labs(fill='') +\n",
    "#  p9.theme(figure_size=(6, 6),\n",
    "#             dpi=300,\n",
    "#             legend_position=(.65, .175)) +\n",
    "# p9.ggtitle('Meteorological Stations grouped by municipality lvl.')\n",
    "#  ).draw())\n",
    "variables = [\"42\", \"44\", \"40\", \"3\"]\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "var_stat_data = pd.DataFrame()\n",
    "\n",
    "for i in variables:\n",
    "    folder_path = f\"data/enironment/{i}\"\n",
    "\n",
    "    # Get a list of all files in the folder\n",
    "    all_files = os.listdir(folder_path)\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Loop through each file and read it into a DataFrame\n",
    "    for file_name in all_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Check if the file is a CSV file (you can adjust the condition for other file types)\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            # Append the DataFrame to the list\n",
    "            dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_var_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Rename the 'value' column to the current variable\n",
    "    combined_var_df.rename(columns={'VALOR_LECTURA': variables_legend.loc[variables_legend.codi_variable==i].reset_index(drop = True)[\"nom_variable\"][0],\\\n",
    "                                     'CODI_ESTACIO' : 'codi_estacio',\\\n",
    "                                     'DATA_LECTURA': 'date'}, inplace=True)\n",
    "    \n",
    "    # Merge the current variable data with the combined data\n",
    "    if var_stat_data.empty:\n",
    "        var_stat_data = combined_var_df\n",
    "    else:\n",
    "        var_stat_data = pd.merge(var_stat_data, combined_var_df, on=['codi_estacio', 'date'], how='outer')\n",
    "\n",
    "# del df, dfs, groupby_year, combined_var_df\n",
    "# Convert 'date' column to datetime format\n",
    "var_stat_data['date'] = pd.to_datetime(var_stat_data['date'], format='%d/%m/%Y %I:%M:%S %p', errors='coerce')\n",
    "\n",
    "# Select columns for which you want to calculate daily averages\n",
    "columns_to_aggregate = {\n",
    "    'Maximum Relative Humidity': 'max',\n",
    "    'Minimum Relative Humidity': 'min',\n",
    "    'Maximum Temperature': 'max',\n",
    "    'Minimum Temperature': 'min'\n",
    "}\n",
    "\n",
    "# Group by date and station, and calculate daily aggregate (maximum or minimum) for selected columns\n",
    "var_stat_data['date'] = var_stat_data['date'].dt.strftime('%d/%m/%Y')\n",
    "var_stat_data = var_stat_data.groupby(['date', 'codi_estacio']).agg(columns_to_aggregate).reset_index()\n",
    "\n",
    "# Convert 'date' column back to datetime format\n",
    "var_stat_data['date'] = pd.to_datetime(var_stat_data['date'], format='%d/%m/%Y')\n",
    "\n",
    "# Extract year from the date\n",
    "var_stat_data['year'] = var_stat_data['date'].dt.year\n",
    "station_com_at = station_legend.rename(columns = {'codi_comarca':'com_code'})[['com_code','codi_estacio']]\\\n",
    "    .merge(pc_muni_com_at_df[['at_code','com_code']].drop_duplicates(), on = 'com_code')\n",
    "var_stat_data_at_com = var_stat_data.merge(station_com_at, on = 'codi_estacio').drop_duplicates().sort_values(['codi_estacio','date']).reset_index(drop = True)\n",
    "var_stat_data_at_com[['date','at_code']].drop_duplicates() #580018 \n",
    "var_stat_data_at = var_stat_data_at_com.groupby(['date', 'at_code']).agg(columns_to_aggregate).reset_index()\n",
    "var_stat_data_at['daily_diff_temp'] = abs(var_stat_data_at['Maximum Temperature'] - var_stat_data_at['Minimum Temperature'])\n",
    "var_stat_data_at['daily_diff_hum'] = abs(var_stat_data_at['Maximum Relative Humidity'] - var_stat_data_at['Minimum Relative Humidity'])\n",
    "\n",
    "columns_to_diff = ['Maximum Temperature', 'Minimum Temperature', 'Maximum Relative Humidity', 'Minimum Relative Humidity']\n",
    "\n",
    "for column in columns_to_diff:\n",
    "    var_stat_data_at[f'{column.lower().replace(\" \", \"_\")}_change'] = (\n",
    "        var_stat_data_at\n",
    "        .sort_values(['at_code', 'date'])\n",
    "        .reset_index(drop=True)\n",
    "        .groupby('at_code')[column]\n",
    "        .diff()\n",
    "    )\n",
    "\n",
    "# variable_name = ['Maximum Relative Humidity', 'Minimum Relative Humidity', 'Maximum Temperature', 'Minimum Temperature']\n",
    "# for i in variable_name:\n",
    "#     # ACF, PACF\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "#     # Plot ACF on the first subplot\n",
    "#     acf_plot = plot_acf(var_stat_data_at[i], lags=100, ax=ax1, zero = False)\n",
    "#     ax1.set_title(f'Autocorrelation Function \\n (ACF) of {i}')\n",
    "#     ax1.set_ylim([-0.25,0.25])\n",
    "#     ax1.grid(True)\n",
    "#     ax1.set_xlim([0, 100])\n",
    "\n",
    "#     # Plot PACF on the second subplot\n",
    "#     pacf_plot = plot_pacf(var_stat_data_at[i], lags = 50, ax=ax2, zero = False)\n",
    "#     ax2.set_title(f'Partial Autocorrelation Function \\n (PACF) of {i}')\n",
    "#     ax2.set_ylim([-0.25,0.25])\n",
    "#     ax2.grid(True)\n",
    "#     ax2.set_xlim([0, 50])\n",
    "\n",
    "#     # Adjust layout to prevent overlapping\n",
    "#     plt.tight_layout()\n",
    "\n",
    "#     # Show the plots\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_stat_data_at.to_csv('modelling_data/temp_hum_clean.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
